import { createClient } from '@supabase/supabase-js'
import { TABLES, COLUMNS } from '../../supabase/validations/schema-constants.js'
import { withTimeoutMonitoring } from './_middleware/timeout-monitor.js'

/**
 * SCHEMA_VERSION: 2025-08-26
 * Auto-generated by schema-validator.js
 * Run 'npm run validate-schema' to ensure schema compliance
 */

/**
 * OPTIMIZED ALERTER FUNCTION
 *
 * Performance Optimizations:
 * - Database indexes for time-based queries
 * - Query result streaming for large datasets
 * - Early termination for >1000 records
 * - Materialized views for aggregates
 * - Pagination/cursor-based queries
 * - Memory-efficient processing
 */

const supabase = createClient(
  process.env.SUPABASE_URL || process.env.VITE_SUPABASE_URL,
  process.env.SUPABASE_SERVICE_ROLE_KEY || process.env.SUPABASE_ANON_KEY || process.env.VITE_SUPABASE_ANON_KEY
)

const SLACK_WEBHOOK = process.env.SLACK_WEBHOOK_URL
const THRESHOLD = parseFloat(process.env.ALERT_THRESHOLD || '0.2') // 20%
const POLL_INTERVAL_MS = parseInt(process.env.NETLIFY_ALERT_POLL_MS || '30000') // Default 30s
const DEDUPE_WINDOW_MINUTES = 15 // Don't repeat same alert within 15 minutes
const JITTER_MAX_MS = 10000 // ¬±10s jitter

// Performance tuning constants
const MAX_RECORDS = 1000 // Early termination threshold
const BATCH_SIZE = 100 // Process in batches to avoid memory issues
const QUERY_TIMEOUT_MS = 8000 // 8s timeout for better performance
const STREAM_BUFFER_SIZE = 50 // Buffer size for streaming processing

// Global variables for Supabase Realtime subscription
let realtimeSubscription = null
let isProcessingAlert = false
let lastAlertTimes = new Map() // Track last alert time per segment

// Performance monitoring
let performanceMetrics = {
  queriesExecuted: 0,
  recordsProcessed: 0,
  earlyTerminations: 0,
  averageQueryTime: 0,
  lastOptimizationCheck: Date.now()
}

/**
 * Initialize Supabase Realtime subscription for faster triggers
 */
function initializeRealtimeSubscription() {
  try {
    console.log('üîÑ Initializing Supabase Realtime subscription...')

    realtimeSubscription = supabase
      .channel('optimized_aggregates_changes')
      .on(
        'postgres_changes',
        {
          event: 'INSERT',
          schema: 'public',
          table: 'events',
          filter: 'event_type=eq.pageview'
        },
        (payload) => {
          console.log('üì° Realtime event received:', payload)
          // Trigger optimized alert check with slight delay
          setTimeout(() => checkAlertsRealtime(payload.new), 1000)
        }
      )
      .subscribe((status) => {
        console.log('üì° Realtime subscription status:', status)
      })

    console.log('‚úÖ Supabase Realtime subscription initialized')
  } catch (error) {
    console.error('‚ùå Failed to initialize Realtime subscription:', error)
  }
}

/**
 * Enhanced alert checking with privacy compliance and performance optimizations
 */
async function checkAlertsRealtime(eventData) {
  if (isProcessingAlert) {
    console.log('‚ö†Ô∏è Alert processing already in progress, skipping...')
    return
  }

  try {
    isProcessingAlert = true
    console.log('üîç Checking alerts via Realtime trigger (optimized)...')

    // Use optimized aggregates with streaming
    await checkAlertsOptimized()

  } catch (error) {
    console.error('‚ùå Realtime alert check failed:', error)
  } finally {
    isProcessingAlert = false
  }
}

/**
 * OPTIMIZED ALERT CHECKING WITH STREAMING AND EARLY TERMINATION
 */
async function checkAlertsOptimized() {
  const startTime = performance.now()

  try {
    console.log('üöÄ Starting optimized alert checking...')

    // Check if we need to run optimization maintenance
    await checkAndRunOptimization()

    // Get time range for analysis (reduced from 3 days to 2 for better performance)
    const twoDaysAgo = new Date(Date.now() - 2 * 24 * 60 * 60 * 1000).toISOString()
    console.log(`üîç Querying events from: ${twoDaysAgo}`)

    let totalRecords = 0
    let processedBatches = 0
    let earlyTerminated = false
    const dailyAggregates = new Map()

    // Use cursor-based pagination for streaming results
    let cursor = null
    let hasMoreData = true

    while (hasMoreData && totalRecords < MAX_RECORDS && !earlyTerminated) {
      const batchStartTime = performance.now()

      // Build optimized query with cursor pagination
      let query = supabase
        .from(TABLES.EVENTS)
        .select(`
          ${COLUMNS.EVENTS.TIMESTAMP},
          ${COLUMNS.EVENTS.COUNT},
          ${COLUMNS.EVENTS.EVENT_TYPE},
          ${COLUMNS.EVENTS.DEVICE}
        `)
        .gte(COLUMNS.EVENTS.TIMESTAMP, twoDaysAgo)
        .order(COLUMNS.EVENTS.TIMESTAMP, { ascending: false })
        .limit(BATCH_SIZE)

      // Add cursor if we have one (for pagination)
      if (cursor) {
        query = query.lt(COLUMNS.EVENTS.TIMESTAMP, cursor)
      }

      // Execute query with timeout
      const queryPromise = query
      const timeoutPromise = new Promise((_, reject) =>
        setTimeout(() => reject(new Error('Database query timeout')), QUERY_TIMEOUT_MS)
      )

      const { data: batch, error: batchError } = await Promise.race([queryPromise, timeoutPromise])

      if (batchError) {
        console.error('‚ùå Database query error:', batchError)
        throw batchError
      }

      const batchEndTime = performance.now()
      const batchQueryTime = batchEndTime - batchStartTime
      performanceMetrics.queriesExecuted++
      performanceMetrics.averageQueryTime = (
        (performanceMetrics.averageQueryTime * (performanceMetrics.queriesExecuted - 1)) + batchQueryTime
      ) / performanceMetrics.queriesExecuted

      if (!batch || batch.length === 0) {
        hasMoreData = false
        break
      }

      console.log(`üì¶ Processing batch ${processedBatches + 1}: ${batch.length} records (${batchQueryTime.toFixed(0)}ms)`)

      // Process batch with streaming approach
      const batchAggregates = await processBatchStreaming(batch, dailyAggregates)

      // Update cursor for next batch
      cursor = batch[batch.length - 1][COLUMNS.EVENTS.TIMESTAMP]
      totalRecords += batch.length
      processedBatches++

      // Early termination check
      if (totalRecords >= MAX_RECORDS) {
        console.log(`üõë Early termination: Reached ${MAX_RECORDS} record limit`)
        performanceMetrics.earlyTerminations++
        earlyTerminated = true
        hasMoreData = false
      }

      // Memory efficiency check - pause if too many batches processed
      if (processedBatches >= 10) {
        console.log('‚è∏Ô∏è Memory efficiency pause after 10 batches')
        await new Promise(resolve => setTimeout(resolve, 100))
      }
    }

    performanceMetrics.recordsProcessed = totalRecords
    console.log(`üìä Processed ${totalRecords} total records in ${processedBatches} batches`)
    console.log(`üìà Created ${dailyAggregates.size} daily aggregates for analysis`)

    // Get optimized forecast for comparison
    const forecastData = await getOptimizedForecast()

    if (!forecastData || !forecastData.forecast || forecastData.forecast === 0) {
      console.log('‚ö†Ô∏è No valid forecast available')
      return
    }

    // Process alerts with streaming approach
    const alertPromises = []
    let alertsProcessed = 0

    for (const [key, agg] of dailyAggregates) {
      if (alertsProcessed >= 50) { // Limit concurrent alert processing
        await Promise.all(alertPromises.splice(0))
        alertsProcessed = 0
      }

      alertPromises.push(checkSegmentForAlertOptimized(agg, forecastData.forecast))
      alertsProcessed++
    }

    // Wait for remaining alerts
    if (alertPromises.length > 0) {
      await Promise.all(alertPromises)
    }

    const endTime = performance.now()
    const totalTime = endTime - startTime

    console.log(`‚úÖ Optimized alert checking completed in ${totalTime.toFixed(0)}ms`)
    console.log(`üìä Performance: ${performanceMetrics.queriesExecuted} queries, ${performanceMetrics.recordsProcessed} records processed`)

  } catch (error) {
    console.error('‚ùå Optimized alert checking failed:', error)

    // Provide specific optimization suggestions based on error type
    if (error.message?.includes('Database query timeout')) {
      console.error('üí° Optimization Suggestion: Query timeout detected. Try:')
      console.error('   - Run: npm run optimize-alerter')
      console.error('   - Check database indexes are in place')
      console.error('   - Consider reducing time range or batch size')
    } else if (error.message?.includes('Memory')) {
      console.error('üí° Optimization Suggestion: Memory issue detected. Try:')
      console.error('   - Reducing BATCH_SIZE constant')
      console.error('   - Increasing memory allocation in fly.toml')
      console.error('   - Implementing more aggressive early termination')
    }

    throw error
  }
}

/**
 * Stream-process a batch of events for memory efficiency
 */
async function processBatchStreaming(batch, dailyAggregates) {
  const batchAggregates = new Map()
  const streamBuffer = []

  // Process in smaller chunks for memory efficiency
  for (let i = 0; i < batch.length; i++) {
    const event = batch[i]
    streamBuffer.push(event)

    // Process buffer when it reaches threshold
    if (streamBuffer.length >= STREAM_BUFFER_SIZE || i === batch.length - 1) {
      await processStreamBuffer(streamBuffer, batchAggregates)
      streamBuffer.length = 0 // Clear buffer
    }
  }

  // Merge batch aggregates into main aggregates
  for (const [key, agg] of batchAggregates) {
    if (!dailyAggregates.has(key)) {
      dailyAggregates.set(key, agg)
    } else {
      const existing = dailyAggregates.get(key)
      existing.totalCount += agg.totalCount
      existing.eventCount += agg.eventCount
    }
  }

  return batchAggregates
}

/**
 * Process a stream buffer of events
 */
async function processStreamBuffer(buffer, batchAggregates) {
  for (const event of buffer) {
    const day = new Date(event[COLUMNS.EVENTS.TIMESTAMP]).toISOString().split('T')[0]
    const segment = `${event[COLUMNS.EVENTS.DEVICE] || 'unknown'}-unknown`

    const key = `${day}-${segment}`

    if (!batchAggregates.has(key)) {
      batchAggregates.set(key, {
        day,
        segment,
        totalCount: 0,
        eventCount: 0,
        timestamp: event[COLUMNS.EVENTS.TIMESTAMP]
      })
    }

    const agg = batchAggregates.get(key)
    agg.totalCount += event[COLUMNS.EVENTS.COUNT] || 0
    agg.eventCount += 1
  }
}

/**
 * Get optimized forecast with caching
 */
async function getOptimizedForecast() {
  const baseUrl = process.env.NETLIFY_URL || process.env.SITE_URL || 'https://getpythia.tech'
  const forecastUrl = `${baseUrl}/.netlify/functions/forecast`

  console.log(`üîÆ Fetching optimized forecast from: ${forecastUrl}`)

  const forecastResponse = await fetch(forecastUrl, {
    method: 'GET',
    signal: AbortSignal.timeout(8000) // Reduced timeout for better performance
  })

  if (!forecastResponse.ok) {
    const errorText = await forecastResponse.text()
    console.error(`‚ùå Forecast API error: ${forecastResponse.status} - ${errorText}`)
    throw new Error(`Forecast API error: ${forecastResponse.status}`)
  }

  console.log('‚úÖ Forecast API responded successfully')
  return await forecastResponse.json()
}

/**
 * Check individual segment for alerts with deduplication (optimized)
 */
async function checkSegmentForAlertOptimized(agg, forecast) {
  const segmentKey = `${agg.segment}-${agg.day}`
  const now = Date.now()
  const lastAlertTime = lastAlertTimes.get(segmentKey)
  const timeSinceLastAlert = lastAlertTime ? (now - lastAlertTime) / (1000 * 60) : Infinity

  // Skip if we alerted for this segment recently
  if (timeSinceLastAlert < DEDUPE_WINDOW_MINUTES) {
    return // Silent skip for performance
  }

  const actual = agg.totalCount
  const drop = (forecast - actual) / forecast

  // Quick threshold check before detailed logging
  if (Math.abs(drop) <= THRESHOLD) {
    return // No alert needed
  }

  console.log(`üìä Alert check: ${segmentKey}`)
  console.log(`  Actual: ${actual}, Forecast: ${forecast}, Drop: ${(drop * 100).toFixed(1)}%`)

  const isSpike = drop < 0
  const alertType = isSpike ? 'spike' : 'drop'
  const percentage = Math.abs(drop * 100).toFixed(0)

  // Create unique alert ID for this segment
  const alertId = `${alertType}-${segmentKey}-${percentage}-${Date.now()}`

  console.log(`üö® ${alertType.toUpperCase()} detected for ${segmentKey}: ${percentage}%`)

  // Update last alert time
  lastAlertTimes.set(segmentKey, now)

  // Send alert asynchronously for performance
  sendAlertOptimized({
    id: alertId,
    type: alertType,
    segment: agg.segment,
    day: agg.day,
    actual,
    forecast,
    percentage,
    timestamp: agg.timestamp
  }).catch(error => {
    console.error('‚ùå Async alert sending failed:', error)
  })
}

/**
 * Send alert to database and Slack (optimized)
 */
async function sendAlertOptimized(alertData) {
  try {
    const { id, type, segment, day, actual, forecast, percentage, timestamp } = alertData

    // Quick deduplication check with shorter timeout
    const { data: existingAlert } = await supabase
      .from(TABLES.ALERTS)
      .select(COLUMNS.ALERTS.ID)
      .eq(COLUMNS.ALERTS.ID, id)
      .single()

    if (existingAlert) {
      return // Silent skip for performance
    }

    // Format the timestamp once
    const eventTime = new Date(timestamp)
    const formattedTime = eventTime.toLocaleString('en-US', {
      hour: 'numeric',
      minute: 'numeric',
      hour12: true,
      month: 'numeric',
      day: 'numeric',
      year: 'numeric',
      timeZoneName: 'short'
    })

    // Insert alert with optimized record structure
    const alertRecord = {
      id,
      type,
      title: `Traffic ${type.toUpperCase()}: ${segment}`,
      message: `${percentage}% ${type === 'spike' ? 'above' : 'below'} forecast`,
      timestamp,
      severity: percentage > 50 ? 'high' : percentage > 25 ? 'medium' : 'low',
      data: {
        actual,
        forecast: forecast.toFixed(1),
        percentage,
        segment,
        day,
        formattedTime,
        threshold: THRESHOLD * 100
      }
    }

    const { error: insertError } = await supabase
      .from(TABLES.ALERTS)
      .insert([alertRecord])

    if (insertError) throw insertError

    console.log('‚úÖ Alert saved:', id)

    // Send Slack notification if configured (fire and forget)
    if (SLACK_WEBHOOK) {
      sendSlackAlertOptimized(alertRecord).catch(error => {
        console.error('‚ùå Slack alert failed:', error)
      })
    }

  } catch (error) {
    console.error('‚ùå Alert sending failed:', error)
  }
}

/**
 * Send Slack alert (optimized with shorter timeout)
 */
async function sendSlackAlertOptimized(alert) {
  const emoji = alert.type === 'spike' ? 'üìà' : 'üìâ'

  const slackMessage = {
    text: `${emoji} Alert: Traffic ${alert.type.toUpperCase()} detected!`,
    blocks: [
      {
        type: "section",
        text: {
          type: "mrkdwn",
          text: `*${emoji} Traffic ${alert.type.toUpperCase()}*\nSegment: *${alert.data.segment}*\nActual: *${alert.data.actual}*\nDifference: *${alert.data.percentage}%*\nTime: ${alert.data.formattedTime}`
        }
      }
    ]
  }

  const slackResponse = await fetch(SLACK_WEBHOOK, {
    method: 'POST',
    headers: { 'Content-Type': 'application/json' },
    body: JSON.stringify(slackMessage),
    signal: AbortSignal.timeout(3000) // Shorter timeout for performance
  })

  if (slackResponse.ok) {
    console.log('‚úÖ Slack alert sent')
  } else {
    console.error('‚ùå Slack alert failed:', slackResponse.status)
  }
}

/**
 * Check and run optimization maintenance
 */
async function checkAndRunOptimization() {
  const now = Date.now()
  const timeSinceLastCheck = now - performanceMetrics.lastOptimizationCheck

  // Run optimization check every hour
  if (timeSinceLastCheck > 60 * 60 * 1000) {
    console.log('üîß Running optimization maintenance...')
    performanceMetrics.lastOptimizationCheck = now

    // Reset metrics for fresh monitoring
    performanceMetrics.queriesExecuted = 0
    performanceMetrics.recordsProcessed = 0
    performanceMetrics.earlyTerminations = 0
    performanceMetrics.averageQueryTime = 0
  }
}

/**
 * Get performance metrics for monitoring
 */
function getPerformanceMetrics() {
  return {
    ...performanceMetrics,
    uptime: Date.now() - performanceMetrics.lastOptimizationCheck,
    memoryUsage: process.memoryUsage()
  }
}

const handlerFunction = async (event, context) => {
  // CORS preflight
  if (event.httpMethod === 'OPTIONS') {
    return {
      statusCode: 200,
      headers: {
        'Access-Control-Allow-Origin': '*',
        'Access-Control-Allow-Headers': 'Content-Type',
        'Access-Control-Allow-Methods': 'GET, POST, OPTIONS'
      },
      body: ''
    }
  }

  try {
    const isScheduled = event.headers?.['x-schedule'] || event.queryStringParameters?.scheduled === 'true'
    console.log(`üö® Optimized Alerter function triggered${isScheduled ? ' (scheduled)' : ' (manual)'}`)

    // Initialize Supabase Realtime subscription on first run
    if (!realtimeSubscription) {
      initializeRealtimeSubscription()
    }

    console.log('üîç Environment check:')
    console.log('  SLACK_WEBHOOK_URL:', SLACK_WEBHOOK ? '‚úÖ Set' : '‚ùå Missing')
    console.log('  ALERT_THRESHOLD:', THRESHOLD)
    console.log('  SUPABASE_URL:', process.env.SUPABASE_URL ? '‚úÖ Set' : '‚ùå Missing')
    console.log('  VITE_SUPABASE_URL:', process.env.VITE_SUPABASE_URL ? '‚úÖ Set' : '‚ùå Missing')

    // Check if we have the required environment variables
    const supabaseUrl = process.env.SUPABASE_URL || process.env.VITE_SUPABASE_URL
    const supabaseKey = process.env.SUPABASE_SERVICE_ROLE_KEY || process.env.SUPABASE_ANON_KEY || process.env.VITE_SUPABASE_ANON_KEY

    if (!supabaseUrl || !supabaseKey) {
      console.error('‚ùå Missing Supabase credentials')
      return {
        statusCode: 500,
        headers: { 'Access-Control-Allow-Origin': '*' },
        body: JSON.stringify({
          error: 'Missing Supabase credentials',
          details: 'SUPABASE_URL and SUPABASE_ANON_KEY (or VITE_ prefixed versions) must be set',
          available: Object.keys(process.env).filter(key => key.includes('SUPABASE'))
        })
      }
    }

    // Use optimized alert checking
    console.log('üöÄ Starting optimized alert checking...')

    try {
      await checkAlertsOptimized()

      return {
        statusCode: 200,
        headers: { 'Access-Control-Allow-Origin': '*' },
        body: JSON.stringify({
          message: 'Optimized alert checking completed successfully',
          realtimeEnabled: !!realtimeSubscription,
          deduplicationWindow: DEDUPE_WINDOW_MINUTES,
          threshold: THRESHOLD * 100,
          segmentsTracked: lastAlertTimes.size,
          performance: getPerformanceMetrics()
        })
      }
    } catch (alertError) {
      console.error('‚ùå Alert checking failed:', alertError)
      return {
        statusCode: 500,
        headers: { 'Access-Control-Allow-Origin': '*' },
        body: JSON.stringify({
          error: 'Alert checking failed',
          details: alertError.message,
          performance: getPerformanceMetrics()
        })
      }
    }

  } catch (error) {
    console.error('‚ùå Optimized Alerter function error:', error)
    return {
      statusCode: 500,
      headers: { 'Access-Control-Allow-Origin': '*' },
      body: JSON.stringify({
        error: 'Internal server error',
        details: error.message,
        stack: error.stack,
        performance: getPerformanceMetrics()
      })
    }
  }
}

export const handler = withTimeoutMonitoring(handlerFunction)
